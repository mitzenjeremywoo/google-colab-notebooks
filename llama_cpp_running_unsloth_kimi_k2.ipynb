{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFDH7xYn1W2TWJYhrHhVrf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ab71bbde4c944564afa5d27a3c611ba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_043c036b24534cb1bc5fc0b57a9305ec",
              "IPY_MODEL_c9af98d47d554eb78a47c3ae2f7ee077",
              "IPY_MODEL_2c18b022407540099156c8c48f0cc1fb"
            ],
            "layout": "IPY_MODEL_9842b7f41ef54e01a457ba297632d502"
          }
        },
        "043c036b24534cb1bc5fc0b57a9305ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b57d5d6f2e414ea196168ec9184496b9",
            "placeholder": "​",
            "style": "IPY_MODEL_daef2b4bedc14e2c91a91f846747118f",
            "value": "llama-2-7b.Q4_K_M.gguf: 100%"
          }
        },
        "c9af98d47d554eb78a47c3ae2f7ee077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc2ce9f453ec4bc4bf439a749f54a90c",
            "max": 4081004224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31e539c0673e40c6a25ab18addce4f78",
            "value": 4081004224
          }
        },
        "2c18b022407540099156c8c48f0cc1fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47dfda03afa74433b8d525443f4cdd26",
            "placeholder": "​",
            "style": "IPY_MODEL_906c83bf639f4c2593cc66d20720d4d4",
            "value": " 4.08G/4.08G [01:02&lt;00:00, 80.9MB/s]"
          }
        },
        "9842b7f41ef54e01a457ba297632d502": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b57d5d6f2e414ea196168ec9184496b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daef2b4bedc14e2c91a91f846747118f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc2ce9f453ec4bc4bf439a749f54a90c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31e539c0673e40c6a25ab18addce4f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47dfda03afa74433b8d525443f4cdd26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "906c83bf639f4c2593cc66d20720d4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitzenjeremywoo/google-colab-notebooks/blob/main/llama_cpp_running_unsloth_kimi_k2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq install build-essential cmake\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "!cmake -B build\n",
        "!cmake --build build --config Release\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdw7ZE3Qq9qQ",
        "outputId": "835194e1-b424-422b-dd6c-20fa9dfa63c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 59944, done.\u001b[K\n",
            "remote: Counting objects: 100% (301/301), done.\u001b[K\n",
            "remote: Compressing objects: 100% (201/201), done.\u001b[K\n",
            "remote: Total 59944 (delta 210), reused 100 (delta 100), pack-reused 59643 (from 3)\u001b[K\n",
            "Receiving objects: 100% (59944/59944), 150.22 MiB | 23.57 MiB/s, done.\n",
            "Resolving deltas: 100% (43413/43413), done.\n",
            "/content/llama.cpp/llama.cpp\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.0.6257\n",
            "-- ggml commit:  b1afcab8\n",
            "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n",
            "-- Configuring done (2.1s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/llama.cpp/llama.cpp/build\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  4%] Built target ggml-base\n",
            "[  4%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[ 10%] Built target ggml-cpu\n",
            "[ 10%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 10%] Built target ggml\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 21%] Built target llama\n",
            "[ 21%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 21%] Built target build_info\n",
            "[ 21%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 27%] Built target common\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 27%] Built target test-tokenizer-0\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 28%] Built target test-sampling\n",
            "[ 28%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 29%] Built target test-grammar-parser\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 30%] Built target test-grammar-integration\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 31%] Built target test-llama-grammar\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 32%] Built target test-chat\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 34%] Built target test-json-schema-to-grammar\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 35%] Built target test-quantize-stats\n",
            "[ 35%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 36%] Built target test-gbnf-validator\n",
            "[ 37%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 37%] Built target test-tokenizer-1-bpe\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 38%] Built target test-tokenizer-1-spm\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 40%] Built target test-chat-parser\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 41%] Built target test-chat-template\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 42%] Built target test-json-partial\n",
            "[ 42%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 43%] Built target test-log\n",
            "[ 43%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 44%] Built target test-regex-partial\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 46%] Built target test-thread-safety\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 47%] Built target test-arg-parser\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
            "[ 49%] Built target test-opt\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 50%] Built target test-gguf\n",
            "[ 50%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 51%] Built target test-backend-ops\n",
            "[ 51%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 52%] Built target test-model-load-cancel\n",
            "[ 52%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 53%] Built target test-autorelease\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 54%] Built target test-barrier\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 55%] Built target test-quantize-fns\n",
            "[ 55%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 56%] Built target test-quantize-perf\n",
            "[ 56%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 57%] Built target test-rope\n",
            "[ 57%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 59%] Built target mtmd\n",
            "[ 60%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 60%] Built target test-mtmd-c-api\n",
            "[ 61%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 61%] Built target test-c\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 62%] Built target llama-batched\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 63%] Built target llama-embedding\n",
            "[ 63%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 63%] Built target llama-eval-callback\n",
            "[ 64%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[ 64%] Built target sha256\n",
            "[ 65%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[ 65%] Built target xxhash\n",
            "[ 65%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[ 65%] Built target sha1\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 66%] Built target llama-gguf-hash\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 66%] Built target llama-gguf\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gritlm\u001b[0m\n",
            "[ 67%] Built target llama-gritlm\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 68%] Built target llama-lookahead\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 69%] Built target llama-lookup\n",
            "[ 69%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 70%] Built target llama-lookup-create\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 70%] Built target llama-lookup-merge\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 71%] Built target llama-lookup-stats\n",
            "[ 71%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 72%] Built target llama-parallel\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 73%] Built target llama-passkey\n",
            "[ 73%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 74%] Built target llama-retrieval\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 75%] Built target llama-save-load-state\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 76%] Built target llama-simple\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 77%] Built target llama-simple-chat\n",
            "[ 77%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 78%] Built target llama-speculative\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 78%] Built target llama-speculative-simple\n",
            "[ 78%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 79%] Built target llama-gen-docs\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 80%] Built target llama-finetune\n",
            "[ 80%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 81%] Built target llama-diffusion-cli\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/model-conversion/CMakeFiles/llama-logits.dir/logits.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-logits\u001b[0m\n",
            "[ 82%] Built target llama-logits\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 83%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 83%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 84%] Built target llama-vdot\n",
            "[ 85%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 85%] Built target llama-q8dot\n",
            "[ 85%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 86%] Built target llama-batched-bench\n",
            "[ 87%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 87%] Built target llama-gguf-split\n",
            "[ 87%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 88%] Built target llama-imatrix\n",
            "[ 88%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 89%] Built target llama-bench\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/main/CMakeFiles/llama-cli.dir/main.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 89%] Built target llama-cli\n",
            "[ 89%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 89%] Built target llama-perplexity\n",
            "[ 90%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 90%] Built target llama-quantize\n",
            "[ 90%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 90%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[ 91%] Built target llama-server\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/run.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-run\u001b[0m\n",
            "[ 92%] Built target llama-run\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 93%] Built target llama-tokenize\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 94%] Built target llama-tts\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[ 94%] Built target llama-llava-cli\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[ 95%] Built target llama-gemma3-cli\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[ 96%] Built target llama-minicpmv-cli\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[ 97%] Built target llama-qwen2vl-cli\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 98%] Built target llama-mtmd-cli\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 99%] Built target llama-cvector-generator\n",
            "[100%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[100%] Built target llama-export-lora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Example: download a 7B Q4 model\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"TheBloke/Llama-2-7B-GGUF\",\n",
        "    filename=\"llama-2-7b.Q4_K_M.gguf\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "ab71bbde4c944564afa5d27a3c611ba2",
            "043c036b24534cb1bc5fc0b57a9305ec",
            "c9af98d47d554eb78a47c3ae2f7ee077",
            "2c18b022407540099156c8c48f0cc1fb",
            "9842b7f41ef54e01a457ba297632d502",
            "b57d5d6f2e414ea196168ec9184496b9",
            "daef2b4bedc14e2c91a91f846747118f",
            "bc2ce9f453ec4bc4bf439a749f54a90c",
            "31e539c0673e40c6a25ab18addce4f78",
            "47dfda03afa74433b8d525443f4cdd26",
            "906c83bf639f4c2593cc66d20720d4d4"
          ]
        },
        "id": "zVUU4FgiurRH",
        "outputId": "9f6c6d0f-ac85-4af2-c688-a0c05497dd9e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.8.3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "llama-2-7b.Q4_K_M.gguf:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab71bbde4c944564afa5d27a3c611ba2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name \"llama-server\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLJ3dktuvB-N",
        "outputId": "075591db-960f-42b6-9a77-ce97ca3cafe8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "find: ‘/proc/69/task/69/net’: Invalid argument\n",
            "find: ‘/proc/69/net’: Invalid argument\n",
            "/content/llama.cpp/llama.cpp/build/bin/llama-server\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama.cpp/llama.cpp/build/bin/llama-server -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D1bDkIAv1KE",
        "outputId": "b7896281-5635-4a90-ee57-40352f0bd87a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "--completion-bash                       print source-able bash completion script for llama.cpp\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : low(-1), normal(0), medium(1), high(2),\n",
            "                                        realtime(3) (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "--swa-full                              use full-size SWA cache (default: false)\n",
            "                                        [(more\n",
            "                                        info)](https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
            "                                        (env: LLAMA_ARG_SWA_FULL)\n",
            "--kv-unified, -kvu                      use single unified KV buffer for the KV cache of all sequences\n",
            "                                        (default: false)\n",
            "                                        [(more info)](https://github.com/ggml-org/llama.cpp/pull/14363)\n",
            "                                        (env: LLAMA_ARG_KV_SPLIT)\n",
            "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "--no-perf                               disable internal libllama performance timings (default: false)\n",
            "                                        (env: LLAMA_ARG_NO_PERF)\n",
            "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
            "--no-escape                             do not process escape sequences\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
            "-nkvo, --no-kv-offload                  disable KV offload\n",
            "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
            "-nr,   --no-repack                      disable weight repacking\n",
            "                                        (env: LLAMA_ARG_NO_REPACK)\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (DEPRECATED)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "                                        (env: LLAMA_ARG_MLOCK)\n",
            "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
            "                                        using mlock)\n",
            "                                        (env: LLAMA_ARG_NO_MMAP)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggml-org/llama.cpp/issues/1437\n",
            "                                        (env: LLAMA_ARG_NUMA)\n",
            "-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't\n",
            "                                        offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "                                        (env: LLAMA_ARG_DEVICE)\n",
            "--list-devices                          print list of available devices and exit\n",
            "--override-tensor, -ot <tensor name pattern>=<buffer type>,...\n",
            "                                        override tensor buffer type\n",
            "--cpu-moe, -cmoe                        keep all Mixture of Experts (MoE) weights in the CPU\n",
            "                                        (env: LLAMA_ARG_CPU_MOE)\n",
            "--n-cpu-moe, -ncmoe N                   keep the Mixture of Experts (MoE) weights of the first N layers in the\n",
            "                                        CPU\n",
            "                                        (env: LLAMA_ARG_N_CPU_MOE)\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
            "                                        multiple times.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false\n",
            "--no-op-offload                         disable offloading host tensor operations to device (default: false)\n",
            "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
            "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
            "                                        multiple adapters)\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: this argument can be repeated to add multiple control vectors\n",
            "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
            "                                        note: this argument can be repeated to add multiple scaled control\n",
            "                                        vectors\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
            "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-hf,   -hfr, --hf-repo <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository; quant is optional, case-insensitive,\n",
            "                                        default to Q4_K_M, or falls back to the first file in the repo if\n",
            "                                        Q4_K_M doesn't exist.\n",
            "                                        mmproj is also downloaded automatically if available. to disable, add\n",
            "                                        --no-mmproj\n",
            "                                        example: unsloth/phi-4-GGUF:q4_k_m\n",
            "                                        (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]\n",
            "                                        Same as --hf-repo, but for the draft model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HFD_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in\n",
            "                                        --hf-repo (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO_V)\n",
            "-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE_V)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "--log-colors                            Enable colored logging\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "--offline                               Offline mode: forces use of cache, prevents network access\n",
            "                                        (env: LLAMA_OFFLINE)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored.\n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefix in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "-ctkd, --cache-type-k-draft TYPE        KV cache data type for K for the draft model\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K_DRAFT)\n",
            "-ctvd, --cache-type-v-draft TYPE        KV cache data type for V for the draft model\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V_DRAFT)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default:\n",
            "                                        penalties;dry;top_n_sigma;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
            "--sampling-seq, --sampler-seq SEQUENCE\n",
            "                                        simplified sequence for samplers that will be used (default:\n",
            "                                        edskypmxt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--top-nsigma N                          top-n-sigma sampling (default: -1.0, -1.0 = disabled)\n",
            "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
            "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
            "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
            "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
            "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
            "                                        context size)\n",
            "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
            "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
            "                                        sequence breakers\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "-jf,   --json-schema-file FILE          File containing a JSON schema to constrain generations\n",
            "                                        (https://json-schema.org/), e.g. `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--swa-checkpoints N                     max number of SWA checkpoints per slot to create (default: 3)\n",
            "                                        [(more info)](https://github.com/ggml-org/llama.cpp/pull/15293)\n",
            "                                        (env: LLAMA_ARG_SWA_CHECKPOINTS)\n",
            "--no-context-shift                      disables context shift on infinite text generation (default: enabled)\n",
            "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
            "--context-shift                         enables context shift on infinite text generation (default: disabled)\n",
            "                                        (env: LLAMA_ARG_CONTEXT_SHIFT)\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "--no-warmup                             skip warming up the model with an empty run\n",
            "--spm-infill                            use Suffix/Prefix/Middle pattern for infill (instead of\n",
            "                                        Prefix/Suffix/Middle) as some models prefer this. (default: disabled)\n",
            "--pooling {none,mean,cls,last,rank}     pooling type for embeddings, use model default if unspecified\n",
            "                                        (env: LLAMA_ARG_POOLING)\n",
            "-cb,   --cont-batching                  enable continuous batching (a.k.a dynamic batching) (default: enabled)\n",
            "                                        (env: LLAMA_ARG_CONT_BATCHING)\n",
            "-nocb, --no-cont-batching               disable continuous batching\n",
            "                                        (env: LLAMA_ARG_NO_CONT_BATCHING)\n",
            "--mmproj FILE                           path to a multimodal projector file. see tools/mtmd/README.md\n",
            "                                        note: if -hf is used, this argument can be omitted\n",
            "                                        (env: LLAMA_ARG_MMPROJ)\n",
            "--mmproj-url URL                        URL to a multimodal projector file. see tools/mtmd/README.md\n",
            "                                        (env: LLAMA_ARG_MMPROJ_URL)\n",
            "--no-mmproj                             explicitly disable multimodal projector, useful when using -hf\n",
            "                                        (env: LLAMA_ARG_NO_MMPROJ)\n",
            "--no-mmproj-offload                     do not offload multimodal projector to GPU\n",
            "                                        (env: LLAMA_ARG_NO_MMPROJ_OFFLOAD)\n",
            "--override-tensor-draft, -otd <tensor name pattern>=<buffer type>,...\n",
            "                                        override tensor buffer type for draft model\n",
            "--cpu-moe-draft, -cmoed                 keep all Mixture of Experts (MoE) weights in the CPU for the draft\n",
            "                                        model\n",
            "                                        (env: LLAMA_ARG_CPU_MOE_DRAFT)\n",
            "--n-cpu-moe-draft, -ncmoed N            keep the Mixture of Experts (MoE) weights of the first N layers in the\n",
            "                                        CPU for the draft model\n",
            "                                        (env: LLAMA_ARG_N_CPU_MOE_DRAFT)\n",
            "-a,    --alias STRING                   set alias for model name (to be used by REST API)\n",
            "                                        (env: LLAMA_ARG_ALIAS)\n",
            "--host HOST                             ip address to listen, or bind to an UNIX socket if the address ends\n",
            "                                        with .sock (default: 127.0.0.1)\n",
            "                                        (env: LLAMA_ARG_HOST)\n",
            "--port PORT                             port to listen (default: 8080)\n",
            "                                        (env: LLAMA_ARG_PORT)\n",
            "--path PATH                             path to serve static files from (default: )\n",
            "                                        (env: LLAMA_ARG_STATIC_PATH)\n",
            "--api-prefix PREFIX                     prefix path the server serves from, without the trailing slash\n",
            "                                        (default: )\n",
            "                                        (env: LLAMA_ARG_API_PREFIX)\n",
            "--no-webui                              Disable the Web UI (default: enabled)\n",
            "                                        (env: LLAMA_ARG_NO_WEBUI)\n",
            "--embedding, --embeddings               restrict to only support embedding use case; use only with dedicated\n",
            "                                        embedding models (default: disabled)\n",
            "                                        (env: LLAMA_ARG_EMBEDDINGS)\n",
            "--reranking, --rerank                   enable reranking endpoint on server (default: disabled)\n",
            "                                        (env: LLAMA_ARG_RERANKING)\n",
            "--api-key KEY                           API key to use for authentication (default: none)\n",
            "                                        (env: LLAMA_API_KEY)\n",
            "--api-key-file FNAME                    path to file containing API keys (default: none)\n",
            "--ssl-key-file FNAME                    path to file a PEM-encoded SSL private key\n",
            "                                        (env: LLAMA_ARG_SSL_KEY_FILE)\n",
            "--ssl-cert-file FNAME                   path to file a PEM-encoded SSL certificate\n",
            "                                        (env: LLAMA_ARG_SSL_CERT_FILE)\n",
            "--chat-template-kwargs STRING           sets additional params for the json template parser\n",
            "                                        (env: LLAMA_CHAT_TEMPLATE_KWARGS)\n",
            "-to,   --timeout N                      server read/write timeout in seconds (default: 600)\n",
            "                                        (env: LLAMA_ARG_TIMEOUT)\n",
            "--threads-http N                        number of threads used to process HTTP requests (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS_HTTP)\n",
            "--cache-reuse N                         min chunk size to attempt reusing from the cache via KV shifting\n",
            "                                        (default: 0)\n",
            "                                        [(card)](https://ggml.ai/f0.png)\n",
            "                                        (env: LLAMA_ARG_CACHE_REUSE)\n",
            "--metrics                               enable prometheus compatible metrics endpoint (default: disabled)\n",
            "                                        (env: LLAMA_ARG_ENDPOINT_METRICS)\n",
            "--slots                                 enable slots monitoring endpoint (default: disabled)\n",
            "                                        (env: LLAMA_ARG_ENDPOINT_SLOTS)\n",
            "--props                                 enable changing global properties via POST /props (default: disabled)\n",
            "                                        (env: LLAMA_ARG_ENDPOINT_PROPS)\n",
            "--no-slots                              disables slots monitoring endpoint\n",
            "                                        (env: LLAMA_ARG_NO_ENDPOINT_SLOTS)\n",
            "--slot-save-path PATH                   path to save slot kv cache (default: disabled)\n",
            "--jinja                                 use jinja template for chat (default: disabled)\n",
            "                                        (env: LLAMA_ARG_JINJA)\n",
            "--reasoning-format FORMAT               controls whether thought tags are allowed and/or extracted from the\n",
            "                                        response, and in which format they're returned; one of:\n",
            "                                        - none: leaves thoughts unparsed in `message.content`\n",
            "                                        - deepseek: puts thoughts in `message.reasoning_content` (except in\n",
            "                                        streaming mode, which behaves as `none`)\n",
            "                                        (default: auto)\n",
            "                                        (env: LLAMA_ARG_THINK)\n",
            "--reasoning-budget N                    controls the amount of thinking allowed; currently only one of: -1 for\n",
            "                                        unrestricted thinking budget, or 0 to disable thinking (default: -1)\n",
            "                                        (env: LLAMA_ARG_THINK_BUDGET)\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,\n",
            "                                        deepseek3, exaone3, exaone4, falcon3, gemma, gigachat, glmedge,\n",
            "                                        gpt-oss, granite, hunyuan-dense, hunyuan-moe, kimi-k2, llama2,\n",
            "                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4, megrez,\n",
            "                                        minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7,\n",
            "                                        mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world,\n",
            "                                        seed_oss, smolvlm, vicuna, vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--chat-template-file JINJA_TEMPLATE_FILE\n",
            "                                        set custom jinja chat template file (default: template taken from\n",
            "                                        model's metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,\n",
            "                                        deepseek3, exaone3, exaone4, falcon3, gemma, gigachat, glmedge,\n",
            "                                        gpt-oss, granite, hunyuan-dense, hunyuan-moe, kimi-k2, llama2,\n",
            "                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4, megrez,\n",
            "                                        minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7,\n",
            "                                        mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world,\n",
            "                                        seed_oss, smolvlm, vicuna, vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)\n",
            "--no-prefill-assistant                  whether to prefill the assistant's response if the last message is an\n",
            "                                        assistant message (default: prefill enabled)\n",
            "                                        when this flag is set, if the last message is an assistant message\n",
            "                                        then it will be treated as a full message and not prefilled\n",
            "                                        \n",
            "                                        (env: LLAMA_ARG_NO_PREFILL_ASSISTANT)\n",
            "-sps,  --slot-prompt-similarity SIMILARITY\n",
            "                                        how much the prompt of a request must match the prompt of a slot in\n",
            "                                        order to use that slot (default: 0.50, 0.0 = disabled)\n",
            "--lora-init-without-apply               load LoRA adapters without applying them (apply later via POST\n",
            "                                        /lora-adapters) (default: disabled)\n",
            "-td,   --threads-draft N                number of threads to use during generation (default: same as\n",
            "                                        --threads)\n",
            "-tbd,  --threads-batch-draft N          number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads-draft)\n",
            "--draft-max, --draft, --draft-n N       number of tokens to draft for speculative decoding (default: 16)\n",
            "                                        (env: LLAMA_ARG_DRAFT_MAX)\n",
            "--draft-min, --draft-n-min N            minimum number of draft tokens to use for speculative decoding\n",
            "                                        (default: 0)\n",
            "                                        (env: LLAMA_ARG_DRAFT_MIN)\n",
            "--draft-p-min P                         minimum speculative decoding probability (greedy) (default: 0.8)\n",
            "                                        (env: LLAMA_ARG_DRAFT_P_MIN)\n",
            "-cd,   --ctx-size-draft N               size of the prompt context for the draft model (default: 0, 0 = loaded\n",
            "                                        from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE_DRAFT)\n",
            "-devd, --device-draft <dev1,dev2,..>    comma-separated list of devices to use for offloading the draft model\n",
            "                                        (none = don't offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "-ngld, --gpu-layers-draft, --n-gpu-layers-draft N\n",
            "                                        number of layers to store in VRAM for the draft model\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS_DRAFT)\n",
            "-md,   --model-draft FNAME              draft model for speculative decoding (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_DRAFT)\n",
            "--spec-replace TARGET DRAFT             translate the string in TARGET into DRAFT if the draft model and main\n",
            "                                        model are not compatible\n",
            "-mv,   --model-vocoder FNAME            vocoder model for audio generation (default: unused)\n",
            "--tts-use-guide-tokens                  Use guide tokens to improve TTS word recall\n",
            "--embd-bge-small-en-default             use default bge-small-en-v1.5 model (note: can download weights from\n",
            "                                        the internet)\n",
            "--embd-e5-small-en-default              use default e5-small-v2 model (note: can download weights from the\n",
            "                                        internet)\n",
            "--embd-gte-small-default                use default gte-small model (note: can download weights from the\n",
            "                                        internet)\n",
            "--fim-qwen-1.5b-default                 use default Qwen 2.5 Coder 1.5B (note: can download weights from the\n",
            "                                        internet)\n",
            "--fim-qwen-3b-default                   use default Qwen 2.5 Coder 3B (note: can download weights from the\n",
            "                                        internet)\n",
            "--fim-qwen-7b-default                   use default Qwen 2.5 Coder 7B (note: can download weights from the\n",
            "                                        internet)\n",
            "--fim-qwen-7b-spec                      use Qwen 2.5 Coder 7B + 0.5B draft for speculative decoding (note: can\n",
            "                                        download weights from the internet)\n",
            "--fim-qwen-14b-spec                     use Qwen 2.5 Coder 14B + 0.5B draft for speculative decoding (note:\n",
            "                                        can download weights from the internet)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/llama.cpp/llama.cpp/build/bin/llama-server -hf unsloth/Kimi-K2-Instruct-GGUF:Q4_K_M --host 0.0.0.0 --port 8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Up05zpAv5Rq",
        "outputId": "ed706350-9ba5-437e-8758-0d7eb23a9d84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "common_download_file_single: no previous model file found /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00001-of-00013.gguf\n",
            "curl_perform_with_retry: HEAD https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00001-of-00013.gguf (attempt 1 of 1)...\n",
            "common_download_file_single: trying to download model from https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00001-of-00013.gguf to /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00001-of-00013.gguf (server_etag:\"3695b3e38171ad60eb69132ddba3ca1d8eae30f1d2a5039828981679b25e0246\", server_last_modified:)...\n",
            "curl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00001-of-00013.gguf (attempt 1 of 3)...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1371  100  1371    0     0  25644      0 --:--:-- --:--:-- --:--:-- 25644\n",
            "100 43.0G  100 43.0G    0     0  59.7M      0  0:12:18  0:12:18 --:--:-- 51.7M\n",
            "common_download_file_single: no previous model file found /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00002-of-00013.gguf\n",
            "curl_perform_with_retry: HEAD https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00002-of-00013.gguf (attempt 1 of 1)...\n",
            "common_download_file_single: no previous model file found /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00004-of-00013.gguf\n",
            "common_download_file_single: no previous model file found /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00009-of-00013.gguf\n",
            "curl_perform_with_retry: HEAD https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00004-of-00013.gguf (attempt 1 of 1)...\n",
            "common_download_file_single: no previous model file found /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00007-of-00013.gguf\n",
            "common_download_file_single: no previous model file found /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00006-of-00013.gguf\n",
            "curl_perform_with_retry: HEAD https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00006-of-00013.gguf (attempt 1 of 1)...\n",
            "common_download_file_single: no previous model file found /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00008-of-00013.gguf\n",
            "curl_perform_with_retry: HEAD https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00008-of-00013.gguf (attempt 1 of 1)...\n",
            "curl_perform_with_retry: HEAD https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00007-of-00013.gguf (attempt 1 of 1)...\n",
            "common_download_file_single: no previous model file found /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00003-of-00013.gguf\n",
            "curl_perform_with_retry: HEAD https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00009-of-00013.gguf (attempt 1 of 1)...\n",
            "common_download_file_single: no previous model file found /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00005-of-00013.gguf\n",
            "common_download_file_single: no previous model file found /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00011-of-00013.gguf\n",
            "curl_perform_with_retry: HEAD https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00011-of-00013.gguf (attempt 1 of 1)...\n",
            "curl_perform_with_retry: HEAD https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00003-of-00013.gguf (attempt 1 of 1)...\n",
            "curl_perform_with_retry: HEAD https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00005-of-00013.gguf (attempt 1 of 1)...\n",
            "common_download_file_single: no previous model file found /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00010-of-00013.gguf\n",
            "common_download_file_single: no previous model file found /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00012-of-00013.gguf\n",
            "curl_perform_with_retry: HEAD https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00012-of-00013.gguf (attempt 1 of 1)...\n",
            "curl_perform_with_retry: HEAD https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00010-of-00013.gguf (attempt 1 of 1)...\n",
            "common_download_file_single: no previous model file found /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00013-of-00013.gguf\n",
            "curl_perform_with_retry: HEAD https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00013-of-00013.gguf (attempt 1 of 1)...\n",
            "common_download_file_single: trying to download model from https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00008-of-00013.gguf to /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00008-of-00013.gguf (server_etag:\"19aa44c3a531d8216c26ef51a95f2b3429a52a0764d5a41fc1512094d8f90ac5\", server_last_modified:)...\n",
            "curl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00008-of-00013.gguf (attempt 1 of 3)...\n",
            "common_download_file_single: trying to download model from https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00002-of-00013.gguf to /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00002-of-00013.gguf (server_etag:\"8befaf2c9b10b09f5ac0e417a0700cab29d5e7f6c04e3d8440d4f3f6e5b511eb\", server_last_modified:)...\n",
            "curl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00002-of-00013.gguf (attempt 1 of 3)...\n",
            "common_download_file_single: trying to download model from https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00006-of-00013.gguf to /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00006-of-00013.gguf (server_etag:\"da05a3ae5ff3780dee4c8e903d09cba9eb75b3c976e04809d5a6b0a0311f77a5\", server_last_modified:)...\n",
            "curl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00006-of-00013.gguf (attempt 1 of 3)...\n",
            "common_download_file_single: trying to download model from https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00012-of-00013.gguf to /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00012-of-00013.gguf (server_etag:\"1eeced7d9e950f8fe12347ce49183fa87b89ddc5c1c5793bd3b3a5ea1b7f445c\", server_last_modified:)...\n",
            "curl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00012-of-00013.gguf (attempt 1 of 3)...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1371  100  1371    0     0  25982      0 --:--:-- --:--:-- --:--:-- 25982\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1369  100  1369    0     0  25445      0 --:--:-- --:--:-- --:--:-- 25445\n",
            "common_download_file_single: trying to download model from https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00005-of-00013.gguf to /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00005-of-00013.gguf (server_etag:\"b471e1241042d06e5353c086753e6fb9819366e4d06b1b2dfc7fd4f99d219865\", server_last_modified:)...\n",
            "curl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00005-of-00013.gguf (attempt 1 of 3)...\n",
            "common_download_file_single: trying to download model from https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00010-of-00013.gguf to /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00010-of-00013.gguf (server_etag:\"92b5063dcc29da3008921e8ecf6895e8c0ed1482daa87c95a4b33e0fe6c4f6dc\", server_last_modified:)...\n",
            "curl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00010-of-00013.gguf (attempt 1 of 3)...\n",
            "common_download_file_single: trying to download model from https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00009-of-00013.gguf to /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00009-of-00013.gguf (server_etag:\"01361e247949d200006212af9770027a1514e3882bbc8b92e30524af3647bfe2\", server_last_modified:)...\n",
            "curl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00009-of-00013.gguf (attempt 1 of 3)...\n",
            "common_download_file_single: trying to download model from https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00007-of-00013.gguf to /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00007-of-00013.gguf (server_etag:\"c0e95451aafac2e76622e9f637286900bcd6db0d0850a1f659f7d0e90f8004df\", server_last_modified:)...\n",
            "curl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00007-of-00013.gguf (attempt 1 of 3)...\n",
            "common_download_file_single: trying to download model from https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00004-of-00013.gguf to /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00004-of-00013.gguf (server_etag:\"426499dc3324edc78de864a7d82533b804a81831e38f8d337287f1bb8cf74e57\", server_last_modified:)...\n",
            "curl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00004-of-00013.gguf (attempt 1 of 3)...\n",
            "common_download_file_single: trying to download model from https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00003-of-00013.gguf to /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00003-of-00013.gguf (server_etag:\"5757da1fe15d0edef0fb88ad0ee3395729bf9e88e081bff61e24d0ae42dbaa75\", server_last_modified:)...\n",
            "curl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00003-of-00013.gguf (attempt 1 of 3)...\n",
            "common_download_file_single: trying to download model from https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00013-of-00013.gguf to /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00013-of-00013.gguf (server_etag:\"61c73ccac0c38dbc353d40667bff623fdf15b95064fef51b87a3203d4ba8bb77\", server_last_modified:)...\n",
            "curl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00013-of-00013.gguf (attempt 1 of 3)...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1371  100  1371    0     0  27293      0 --:--:-- --:--:-- --:--:-- 27293\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1377  100  1377    0     0  26228      0 --:--:-- --:--:-- --:--:-- 26228\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1377  100  1377    0     0  25763      0 --:--:-- --:--:-- --:--:-- 25763\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1371  100  1371    0     0  27340      0 --:--:-- --:--:-- --:--:-- 27340\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1373  100  1373    0     0  26699      0 --:--:-- --:--:-- --:--:-- 26699\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1371  100  1371    0     0  25868      0 --:--:-- --:--:-- --:--:-- 25868\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1373  100  1373    0     0  27272      0 --:--:-- --:--:-- --:--:-- 27272\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1375  100  1375    0     0  14656      0 --:--:-- --:--:-- --:--:-- 14656\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1371  100  1371    0     0  13415      0 --:--:-- --:--:-- --:--:-- 13415\n",
            "common_download_file_single: trying to download model from https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00011-of-00013.gguf to /root/.cache/llama.cpp/unsloth_Kimi-K2-Instruct-GGUF_Q4_K_M_Kimi-K2-Instruct-Q4_K_M-00011-of-00013.gguf (server_etag:\"71eca5fc0f82a4292daa6152db8caf7783c0d8e5c5161f150cb46dd8565d4c01\", server_last_modified:)...\n",
            "curl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00011-of-00013.gguf (attempt 1 of 3)...\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1375  100  1375    0     0  25312      0 --:--:-- --:--:-- --:--:-- 25312\n",
            "  2 43.1G    2  994M    0     0  3292k      0  3:49:01  0:05:09  3:43:52     0\n",
            "curl_perform_with_retry: curl_easy_perform() failed: Stream error in the HTTP/2 framing layer, retrying after 1000 milliseconds...\n",
            "  2 44.4G    2 1017M    0     0  3362k      0  3:51:14  0:05:09  3:46:05     0curl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00005-of-00013.gguf (attempt 2 of 3)...\n",
            "100  1373  100  1373    0     0   9333      0 --:--:-- --:--:-- --:--:--  9404\n",
            "  2 44.4G    2 1017M    0     0  3302k      0  3:55:25  0:05:15  3:50:10     0\n",
            "curl_perform_with_retry: curl_easy_perform() failed: Stream error in the HTTP/2 framing layer, retrying after 1000 milliseconds...\n",
            "  2 45.8G    2 1248M    0     0  4045k      0  3:17:52  0:05:16  3:12:36 5932kcurl_perform_with_retry: GET https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF/resolve/main/Q4_K_M/Kimi-K2-Instruct-Q4_K_M-00007-of-00013.gguf (attempt 2 of 3)...\n",
            "100  1371  100  1371    0     0   7558      0 --:--:-- --:--:-- --:--:--  7659\n",
            "  3 41.1G    3 1444M    0     0  4015k      0  2:59:10  0:06:08  2:53:02 2980k^C\n"
          ]
        }
      ]
    }
  ]
}