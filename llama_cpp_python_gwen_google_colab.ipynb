{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1442af515a01438ba0f9c5229506cd80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5adc1bf5f7724c0ca40488d2b76b9930",
              "IPY_MODEL_438649a4ac0e4a39992515e604cc1d2d",
              "IPY_MODEL_f839b3cce4124c41b2fb3f9b4ee38bf9"
            ],
            "layout": "IPY_MODEL_9848de4cda9e48af88c328950480f476"
          }
        },
        "5adc1bf5f7724c0ca40488d2b76b9930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15b16b6da08f4a6db2224fd8d143bdc5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_975a6d9a1da54b98af559f69f316a97b",
            "value": "./Qwen3-0.6B-IQ4_NL.gguf:‚Äá100%"
          }
        },
        "438649a4ac0e4a39992515e604cc1d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a85d7f2ade6409295d2cafc9377a241",
            "max": 381566656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23abefaddf0248d589804e3cd88b0ae3",
            "value": 381566656
          }
        },
        "f839b3cce4124c41b2fb3f9b4ee38bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e111eb43b9a446a18c8a7b45ac970e27",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1d734e56d22f4789826bab65bc5686cd",
            "value": "‚Äá382M/382M‚Äá[00:13&lt;00:00,‚Äá10.0kB/s]"
          }
        },
        "9848de4cda9e48af88c328950480f476": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15b16b6da08f4a6db2224fd8d143bdc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "975a6d9a1da54b98af559f69f316a97b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a85d7f2ade6409295d2cafc9377a241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23abefaddf0248d589804e3cd88b0ae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e111eb43b9a446a18c8a7b45ac970e27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d734e56d22f4789826bab65bc5686cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitzenjeremywoo/google-colab-notebooks/blob/main/llama_cpp_python_gwen_google_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U llama-cpp-python"
      ],
      "metadata": {
        "id": "5UA9AyOqF3AB",
        "outputId": "539c7337-4a6b-4875-bfcf-41b91f77974f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=4503284 sha256=193d5d0678e2950093e80551579a7be6cd7827c89ab971a713220777c84abe4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ],
      "metadata": {
        "id": "oX4DBra2F3AD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama-cpp-python\n",
        "\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "\trepo_id=\"unsloth/Qwen3-0.6B-GGUF\",\n",
        "\tfilename=\"Qwen3-0.6B-IQ4_NL.gguf\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "StQQzcKwF3AI",
        "outputId": "7fc7be4e-4470-4320-c345-755263aa1ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1442af515a01438ba0f9c5229506cd80",
            "5adc1bf5f7724c0ca40488d2b76b9930",
            "438649a4ac0e4a39992515e604cc1d2d",
            "f839b3cce4124c41b2fb3f9b4ee38bf9",
            "9848de4cda9e48af88c328950480f476",
            "15b16b6da08f4a6db2224fd8d143bdc5",
            "975a6d9a1da54b98af559f69f316a97b",
            "0a85d7f2ade6409295d2cafc9377a241",
            "23abefaddf0248d589804e3cd88b0ae3",
            "e111eb43b9a446a18c8a7b45ac970e27",
            "1d734e56d22f4789826bab65bc5686cd"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "./Qwen3-0.6B-IQ4_NL.gguf:   0%|          | 0.00/382M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1442af515a01438ba0f9c5229506cd80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/.cache/huggingface/hub/models--unsloth--Qwen3-0.6B-GGUF/snapshots/50968a4468ef4233ed78cd7c3de230dd1d61a56b/./Qwen3-0.6B-IQ4_NL.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen3-0.6B\n",
            "llama_model_loader: - kv   3:                           general.basename str              = Qwen3-0.6B\n",
            "llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 0.6B\n",
            "llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth\n",
            "llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960\n",
            "llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 1024\n",
            "llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 3072\n",
            "llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654\n",
            "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  27:                          general.file_type u32              = 25\n",
            "llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-0.6B-GGUF/imatrix_unsloth.dat\n",
            "llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-0.6B.txt\n",
            "llama_model_loader: - kv  30:             quantize.imatrix.entries_count u32              = 196\n",
            "llama_model_loader: - kv  31:              quantize.imatrix.chunks_count u32              = 688\n",
            "llama_model_loader: - type  f32:  113 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llama_model_loader: - type iq4_nl:  196 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = IQ4_NL - 4.5 bpw\n",
            "print_info: file size   = 358.21 MiB (5.04 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
            "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
            "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
            "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
            "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
            "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
            "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
            "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
            "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
            "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
            "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
            "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
            "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 151643 ('<|endoftext|>')\n",
            "load:   - 151645 ('<|im_end|>')\n",
            "load:   - 151662 ('<|fim_pad|>')\n",
            "load:   - 151663 ('<|repo_name|>')\n",
            "load:   - 151664 ('<|file_sep|>')\n",
            "load: special tokens cache size = 26\n",
            "load: token to piece cache size = 0.9311 MB\n",
            "print_info: arch             = qwen3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 40960\n",
            "print_info: n_embd           = 1024\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 16\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 2\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 3072\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = -1\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 40960\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 0.6B\n",
            "print_info: model params     = 596.05 M\n",
            "print_info: general.name     = Qwen3-0.6B\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 11 ','\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151654 '<|vision_pad|>'\n",
            "print_info: LF token         = 198 'ƒä'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q6_K) (and 114 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_REPACK model buffer size =   236.25 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   356.53 MiB\n",
            "repack: repack tensor blk.0.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.0.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.0.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.0.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.0.ffn_gate.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.0.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.0.ffn_up.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.1.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.1.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.1.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.1.attn_output.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.1.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.1.ffn_down.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.1.ffn_up.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.2.attn_q.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.2.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.2.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.2.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.2.ffn_gate.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.2.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.2.ffn_up.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.3.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.3.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.3.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.3.attn_output.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.3.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.3.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.3.ffn_up.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.4.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.4.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.4.attn_v.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.4.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.4.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.4.ffn_down.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.4.ffn_up.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.5.attn_q.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.5.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.5.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.5.ffn_gate.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.5.ffn_up.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.6.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.6.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.6.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.6.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.6.ffn_down.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.6.ffn_up.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.7.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.7.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.7.attn_v.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.7.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.7.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.7.ffn_down.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.7.ffn_up.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.8.attn_q.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.8.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.8.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.8.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.8.ffn_gate.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.8.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.8.ffn_up.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.9.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.9.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.9.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.9.attn_output.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.9.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.9.ffn_down.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.9.ffn_up.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.10.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.10.attn_k.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.10.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.10.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.10.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.10.ffn_down.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.10.ffn_up.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.11.attn_q.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.11.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.11.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.11.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.11.ffn_gate.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.11.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.11.ffn_up.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.12.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.12.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.12.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.12.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.12.ffn_down.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.12.ffn_up.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.13.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.13.attn_k.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.13.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.13.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.13.ffn_gate.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.13.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.13.ffn_up.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.14.attn_q.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.14.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.14.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.14.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.14.ffn_gate.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.14.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.14.ffn_up.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.15.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.15.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.15.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.15.attn_output.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.15.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.15.ffn_down.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.16.attn_q.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.16.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.16.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.16.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.16.ffn_gate.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.16.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.16.ffn_up.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.17.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.17.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.17.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.17.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.17.ffn_gate.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.17.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.17.ffn_up.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.18.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.18.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.18.attn_v.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.18.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.18.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.18.ffn_down.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.18.ffn_up.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.19.attn_q.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.19.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.19.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.19.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.19.ffn_gate.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.19.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.19.ffn_up.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.20.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.20.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.20.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.20.attn_output.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.20.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.20.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.20.ffn_up.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.21.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.21.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.21.attn_v.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.21.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.21.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.21.ffn_down.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.21.ffn_up.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.22.attn_q.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.22.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.22.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.22.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.22.ffn_gate.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.22.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.22.ffn_up.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.23.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.23.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.23.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.23.attn_output.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.23.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.23.ffn_down.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.23.ffn_up.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.24.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.24.attn_k.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.24.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.24.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.24.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.24.ffn_down.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.24.ffn_up.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.25.attn_q.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.25.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.25.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.25.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.25.ffn_gate.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.25.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.25.ffn_up.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.26.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.26.attn_k.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.26.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.26.attn_output.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.26.ffn_gate.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.26.ffn_down.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.26.ffn_up.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.27.attn_q.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.27.attn_k.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.27.attn_v.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.27.attn_output.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.27.ffn_gate.weight with iq4_nl_8x8\n",
            ".repack: repack tensor blk.27.ffn_down.weight with iq4_nl_8x8\n",
            "repack: repack tensor blk.27.ffn_up.weight with iq4_nl_8x8\n",
            "..\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 512\n",
            "llama_context: n_ctx_per_seq = 512\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.58 MiB\n",
            "create_memory: n_ctx = 512 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =    56.00 MiB\n",
            "llama_kv_cache_unified: size =   56.00 MiB (   512 cells,  28 layers,  1/1 seqs), K (f16):   28.00 MiB, V (f16):   28.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 2480\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   298.75 MiB\n",
            "llama_context: graph nodes  = 1098\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'quantize.imatrix.entries_count': '196', 'quantize.imatrix.dataset': 'unsloth_calibration_Qwen3-0.6B.txt', 'general.file_type': '25', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.eos_token_id': '151645', 'qwen3.attention.value_length': '128', 'quantize.imatrix.chunks_count': '688', 'quantize.imatrix.file': 'Qwen3-0.6B-GGUF/imatrix_unsloth.dat', 'qwen3.attention.key_length': '128', 'general.architecture': 'qwen3', 'tokenizer.ggml.padding_token_id': '151654', 'general.basename': 'Qwen3-0.6B', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\\\n\\\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0].content + \\'<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for forward_message in messages %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- set message = messages[index] %}\\n    {%- set current_content = message.content if message.content is defined and message.content is not none else \\'\\' %}\\n    {%- set tool_start = \\'<tool_response>\\' %}\\n    {%- set tool_start_length = tool_start|length %}\\n    {%- set start_of_message = current_content[:tool_start_length] %}\\n    {%- set tool_end = \\'</tool_response>\\' %}\\n    {%- set tool_end_length = tool_end|length %}\\n    {%- set start_pos = (current_content|length) - tool_end_length %}\\n    {%- if start_pos < 0 %}\\n        {%- set start_pos = 0 %}\\n    {%- endif %}\\n    {%- set end_of_message = current_content[start_pos:] %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {%- set m_content = message.content if message.content is defined and message.content is not none else \\'\\' %}\\n        {%- set content = m_content %}\\n        {%- set reasoning_content = \\'\\' %}\\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if \\'</think>\\' in m_content %}\\n                {%- set content = (m_content.split(\\'</think>\\')|last).lstrip(\\'\\\\n\\') %}\\n                {%- set reasoning_content = (m_content.split(\\'</think>\\')|first).rstrip(\\'\\\\n\\') %}\\n                {%- set reasoning_content = (reasoning_content.split(\\'<think>\\')|last).lstrip(\\'\\\\n\\') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and (not reasoning_content.strip() == \\'\\')) %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n<think>\\\\n\\' + reasoning_content.strip(\\'\\\\n\\') + \\'\\\\n</think>\\\\n\\\\n\\' + content.lstrip(\\'\\\\n\\') }}\\n            {%- else %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- \\'<think>\\\\n\\\\n</think>\\\\n\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}', 'qwen3.context_length': '40960', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen3-0.6B', 'qwen3.rope.freq_base': '1000000.000000', 'general.quantized_by': 'Unsloth', 'qwen3.attention.head_count_kv': '8', 'qwen3.attention.layer_norm_rms_epsilon': '0.000001', 'general.type': 'model', 'general.size_label': '0.6B', 'qwen3.block_count': '28', 'general.repo_url': 'https://huggingface.co/unsloth', 'qwen3.feed_forward_length': '3072', 'qwen3.embedding_length': '1024', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen3.attention.head_count': '16'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0].role == 'system' %}\n",
            "        {{- messages[0].content + '\\n\\n' }}\n",
            "    {%- endif %}\n",
            "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0].role == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
            "{%- for forward_message in messages %}\n",
            "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
            "    {%- set message = messages[index] %}\n",
            "    {%- set current_content = message.content if message.content is defined and message.content is not none else '' %}\n",
            "    {%- set tool_start = '<tool_response>' %}\n",
            "    {%- set tool_start_length = tool_start|length %}\n",
            "    {%- set start_of_message = current_content[:tool_start_length] %}\n",
            "    {%- set tool_end = '</tool_response>' %}\n",
            "    {%- set tool_end_length = tool_end|length %}\n",
            "    {%- set start_pos = (current_content|length) - tool_end_length %}\n",
            "    {%- if start_pos < 0 %}\n",
            "        {%- set start_pos = 0 %}\n",
            "    {%- endif %}\n",
            "    {%- set end_of_message = current_content[start_pos:] %}\n",
            "    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\n",
            "        {%- set ns.multi_step_tool = false %}\n",
            "        {%- set ns.last_query_index = index %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {%- set m_content = message.content if message.content is defined and message.content is not none else '' %}\n",
            "        {%- set content = m_content %}\n",
            "        {%- set reasoning_content = '' %}\n",
            "        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n",
            "            {%- set reasoning_content = message.reasoning_content %}\n",
            "        {%- else %}\n",
            "            {%- if '</think>' in m_content %}\n",
            "                {%- set content = (m_content.split('</think>')|last).lstrip('\\n') %}\n",
            "                {%- set reasoning_content = (m_content.split('</think>')|first).rstrip('\\n') %}\n",
            "                {%- set reasoning_content = (reasoning_content.split('<think>')|last).lstrip('\\n') %}\n",
            "            {%- endif %}\n",
            "        {%- endif %}\n",
            "        {%- if loop.index0 > ns.last_query_index %}\n",
            "            {%- if loop.last or (not loop.last and (not reasoning_content.strip() == '')) %}\n",
            "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
            "            {%- else %}\n",
            "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
            "            {%- endif %}\n",
            "        {%- else %}\n",
            "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
            "        {%- endif %}\n",
            "        {%- if message.tool_calls %}\n",
            "            {%- for tool_call in message.tool_calls %}\n",
            "                {%- if (loop.first and content) or (not loop.first) %}\n",
            "                    {{- '\\n' }}\n",
            "                {%- endif %}\n",
            "                {%- if tool_call.function %}\n",
            "                    {%- set tool_call = tool_call.function %}\n",
            "                {%- endif %}\n",
            "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
            "                {{- tool_call.name }}\n",
            "                {{- '\", \"arguments\": ' }}\n",
            "                {%- if tool_call.arguments is string %}\n",
            "                    {{- tool_call.arguments }}\n",
            "                {%- else %}\n",
            "                    {{- tool_call.arguments | tojson }}\n",
            "                {%- endif %}\n",
            "                {{- '}\\n</tool_call>' }}\n",
            "            {%- endfor %}\n",
            "        {%- endif %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
            "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: ,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.create_chat_completion(\n",
        "\tmessages = [\n",
        "\t\t{\n",
        "\t\t\t\"role\": \"user\",\n",
        "\t\t\t\"content\": \"What is the capital of France?\"\n",
        "\t\t}\n",
        "\t]\n",
        ")"
      ],
      "metadata": {
        "id": "jlTmoZQLF3AK",
        "outputId": "6d4873dc-7a4d-4063-aa3a-29da15790482",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =     328.78 ms\n",
            "llama_perf_context_print: prompt eval time =     328.53 ms /    15 tokens (   21.90 ms per token,    45.66 tokens per second)\n",
            "llama_perf_context_print:        eval time =    7338.64 ms /   113 runs   (   64.94 ms per token,    15.40 tokens per second)\n",
            "llama_perf_context_print:       total time =    7839.48 ms /   128 tokens\n",
            "llama_perf_context_print:    graphs reused =        109\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-438ef488-a206-4d39-ae79-838c9a94f728',\n",
              " 'object': 'chat.completion',\n",
              " 'created': 1755972835,\n",
              " 'model': '/root/.cache/huggingface/hub/models--unsloth--Qwen3-0.6B-GGUF/snapshots/50968a4468ef4233ed78cd7c3de230dd1d61a56b/./Qwen3-0.6B-IQ4_NL.gguf',\n",
              " 'choices': [{'index': 0,\n",
              "   'message': {'role': 'assistant',\n",
              "    'content': \"<think>\\nOkay, so the user is asking about the capital of France. I need to recall the correct answer. I know that France's capital is Paris. Let me double-check to make sure I'm not mixing up with other countries. For example, in the United States, the capital is Washington D.C., so that's a different country. France's capital is indeed Paris. I think that's right. No need to mention anything else here. Just confirm the answer once more to avoid any confusion.\\n</think>\\n\\nThe capital of France is **Paris**.\"},\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 15, 'completion_tokens': 113, 'total_tokens': 128}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}